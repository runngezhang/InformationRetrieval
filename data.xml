<results>
  <item>
    <title>Online multi-label active annotation: towards large-scale content-based video search</title>
	<abstract>Existing video search engines have not taken the advantages of video content analysis and semantic understanding. Video search in academia uses semantic annotation to approach content-based indexing. We argue this is a promising direction to enable real content-based video search. However, due to the complexity of both video data and semantic concepts, existing techniques on automatic video annotation are still not able to handle large-scale video set and large-scale concept set, in terms of both annotation accuracy and computation cost. To address this problem, in this paper, we propose a scalable framework for annotation-based video search, as well as a novel approach to enable large-scale semantic concept annotation, that is, online multi-label active learning. This framework is scalable to both the video sample dimension and concept label dimension. Large-scale unlabeled video samples are assumed to arrive consecutively in batches with an initial pre-labeled training set, based on which a preliminary multi-label classifier is built. For each arrived batch, a multi-label active learning engine is applied, which automatically selects and manually annotates a set of unlabeled sample-label pairs. And then an online learner updates the original classifier by taking the newly labeled sample-label pairs into consideration. This process repeats until all data are arrived. During the process, new , even without any pre-labeled training samples, can be incorporated into the process anytime. Experiments on TRECVID dataset demonstrate the effectiveness and efficiency of the proposed framework.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An annotation method and application for video contents based on a semantic graph</title>
	<abstract>The progress of digitization at broadcasting stations enables the intellectual production of high-quality contents from video materials. Metadata plays an important role in describing the semantics of multimedia contents by aiding in the semantic structuralization of video material. In this paper, we propose an annotation method for video contents based on the concept of a semantic graph, which allows semantic queries on video contents. This method incorporates three ideas: an annotation for metadata on video scenes; time-series links between video scenes; and semantic links to external data. In the proposed annotation method, metadata are provided as human annotations practical for TV productions. We also present the application of a semantic graph to real baseball game contents and evaluate this application to show the usefulness of the proposed annotation method.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Hidden Markov models for automatic annotation and content-based retrieval of images and video</title>
	<abstract>This paper introduces a novel method for automatic annotation of images with keywords from a generic vocabulary of concepts or objects for the purpose of content-based image retrieval. An image, represented as sequence of feature-vectors characterizing low-level visual features such as color, texture or oriented-edges, is modeled as having been stochastically generated by a hidden Markov model, whose states represent concepts. The parameters of the model are estimated from a set of manually annotated (training) images. Each image in a large test collection is then automatically annotated with the a posteriori probability of concepts present in it. This annotation supports content-based search of the image-collection via keywords. Various aspects of model parameterization, parameter estimation, and image annotation are discussed. Empirical retrieval results are presented on two image-collections | COREL and key-frames from TRECVID. Comparisons are made with two other recently developed techniques on the same datasets.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Video Annotation for Content-based Retrieval using Human Behavior Analysis and Domain Knowledge</title>
	<abstract>This paper proposes an automatic annotation method of sports video for content-based retrieval. Conventional methods using position information of objects such as locus, relative positions, their transitions, etc. as indices, have drawbacks that tracking errors of a certain object due to occlusions cause recognition failures, and that representation by position information essentially has limited number of recognizable events in the retrieval. Our approach incorporates human behavior analysis and specific domain knowledge with conventional methods, to develop integrated reasoning module for richer expressiveness of events and robust recognition.Based on the proposed method, we implemented content-based retrieval system which can identify several actions on real tennis video. We select court and net lines, players' positions, ball positions, and players' actions, as indices. Court and net lines are extracted using court model and hough transforms. Players and ball positions are tracked by adaptive template matching and particular predictions against sudden changes of motion direction. Players' actions are analyzed by 2-d appearance based matching using the transition of players' silhouettes and hidden markov model. The results using two sets of tennis video is presented demonstrating the performance and the validity of our approach.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Knowledge-Supported Segmentation and Semantic Contents Extraction from MPEG Videos for Highlight-Based Annotation, Indexing and Retrieval</title>
	<abstract>Automatic recognition of highlights from videos is a fundamental and challenging problem for content-based indexing and retrieval applications. In this paper, we propose techniques to solve this problem by using knowledge supported extraction of semantic contents, and compressed-domain processing is employed for efficiency. Firstly, video shots are detected by using knowledge-supported rules. Then, human objects are detected via statistical skin detection. Meanwhile, camera motion like zoom in is identified. Finally, highlights of zooming in human objects are extracted and used for annotation, indexing and retrieval of the whole videos. Results from large data of test videos have demonstrated the accuracy and robustness of the proposed techniques.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Video-Based sign language content annotation by incorporation of MPEG-7 standard</title>
	<abstract>The advanced progress in multimedia technology increases the demand on delivering effective content in term of quality with the ability to describe content. From the W3C initiative into the web accessibility (WAI), there is a dedicated effort to make data accessible by every person even by people with disabilities. Accordingly, this paper balances the portion between the minimum bandwidth and the optimum required data to display customized video-based sign language. It also describes a systematic approach derived from the MPEG-7 multimedia content description standard to annotate sign language information. A new approach is proposed by this paper. It makes use of an &#8220;intermediary&#8221; signage object rather than immediate transmission of sign language video clips. Based on the signage object, this research analyses the components in order to enhance the display quality for video-based sign language with less data consumption by determining the accurate display parameters.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Graph aggregation based image modeling and indexing for video annotation</title>
	<abstract>With the rapid growth of video multimedia databases and the lack of textual descriptions for many of them, video annotation became a highly desired task. Conventional systems try to annotate a video query by simply finding its most similar videos in the database. Although the video annotation problem has been tackled in the last decade, no attention has been paid to the problem of assembling video keyframes in a sensed way to provide an answer of the given video query when no single candidate video turns out to be similar to the query. In this paper, we introduce a graph based image modeling and indexing system for video annotation. Our system is able to improve the video annotation task by assembling a set of graphs representing different keyframes of different videos, to compose the video query. The experimental results demonstrate the effectiveness of our system to annotate videos that are not possibly annotated by classical approaches.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Semantic Web for Content Based Video Retrieval</title>
	<abstract>This paper aims to provide a semantic web based video search engine. Currently, we do not have scalable integration platforms to represent extracted features from videos, so that they could be indexed and searched. The task of indexing extracted features from videos is a difficult challenge, due to the diverse nature of the features and the temporal dimensions of videos. We present a semantic web based framework for automatic feature extraction, storage, indexing and retrieval of videos. Videos are represented as interconnected set of semantic resources. Also, we suggest a new ranking algorithm for finding related resources which could be used in a semantic web based search engine.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Extracting Motion Annotations from MPEG-2 Compressed Video for HDTV Content Management Applications</title>
	<abstract>Our research concentrates on developing a novel HDTV content management system that enables end users to search, retrieve, and browse archived standard definition (SD) and high definition (HD) television material for program production and content repurposing in a digital television studio. We have developed the first system that automatically analyzes motion occurring in MPEG-2 coded SD and HD videos within the compressed domain itself, and produces descriptors characterizing the global visual motion in videos, for content-based search and retrieval applications. In this paper, we describe our robust and efficient scheme for automatically generating a flow characterization of a video bitstream without decompression, which is a frame-type-independent uniform motion representation amenable for consistent interpretation and computed from the raw motion vectors encoded in the MPEG-2 bitstreams. We propose novel techniques to handle all the different prediction schemes that are employed with different frame types and picture structures of MPEG-2 during the motion compensation process to deal with interlaced and progressive modes. Experiments with thousands of frames from SD and HD video streams demonstrate the accuracy of our flow estimation process and the effectiveness of utilizing flow vectors for annotation of perceived global motion in video streams.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Film Video Modeling</title>
	<abstract>In this paper we present our concept and design of a general film model that represents structural, semantic, and syntactic elements of film. The purpose of this model is to serve as a basis for film video annotation and subsequent retrieval. It is informed by contemporary film theory and analysis to adequately represent the film content and its attributes. The implementation of such a model enables users to access a much vaster array of multimodal film content than is presently possible.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Classification of Tennis Video for High-level Content-based Retrieval</title>
	<abstract>This paper presents our techniques and results on automatic analysis of tennis video to facilitate content-based retrieval. Our approach is based on the generation of an image model for the tennis court-lines. We derive this model by using the knowledge about dimensions and connectivity (form) of a tennis court and typical camera geometry used when capturing a tennis video. We use this model to develop (i) a court line detection algorithm and (ii) a robust player-tracking algorithm to track the tennis players over the images sequence. We also present a color-based algorithm to select tennis court clips from an input raw footage of tennis video. Automatically extracted tennis court lines and the players' location information are analyzed in a high-level reasoning module and related to useful high-level tennis play events. Results on real tennis video data are presented demonstrating the validity and performance of the approach.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Active learning in multimedia annotation and retrieval: A survey</title>
	<abstract>Active learning is a machine learning technique that selects the most informative samples for labeling and uses them as training data. It has been widely explored in multimedia research community for its capability of reducing human annotation effort. In this article, we provide a survey on the efforts of leveraging active learning in multimedia annotation and retrieval. We mainly focus on two application domains: image/video annotation and content-based image retrieval. We first briefly introduce the principle of active learning and then we analyze the sample selection criteria. We categorize the existing sample selection strategies used in multimedia annotation and retrieval into five criteria: risk reduction, uncertainty, diversity, density and relevance. We then introduce several classification models used in active learning-based multimedia annotation and retrieval, including semi-supervised learning, multilabel learning and multiple instance learning. We also provide a discussion on several future trends in this research direction. In particular, we discuss cost analysis of human annotation and large-scale interactive multimedia annotation.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Optimizing multi-graph learning: towards a unified video annotation scheme</title>
	<abstract>Learning based semantic video annotation is a promising approach for enabling content-based video search. However, severe difficulties, such as insufficiency of training data and curse of dimensionality, are frequently encountered. This paper proposes a novel unified scheme, Optimized Multi-Graph-based Semi-Supervised Learning (OMG-SSL), to simultaneously attack these difficulties. Instead of only using a single graph, OMG-SSL integrates multiple graphs into a regularization and optimization framework to sufficiently explore their complementary nature. We then show that various crucial factors in video annotation, including multiple modalities, multiple distance metrics, and temporal consistency, in fact all correspond to different correlations among samples, and hence they can be represented by different graphs. Therefore, OMG-SSL is able to simultaneously deal with these factors within a unified framework. Experiments on the TRECVID benchmark demonstrate the effectiveness of our proposed approach.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Visual video retrieval system using MPEG-7 descriptors</title>
	<abstract>Video is one of the most popular media these days. However, the volume of the video content grows very fast, e.g. about 24 hours of video content is uploaded every minute to the most famous web site YouTube, and the necessity to search in this content is apparent. Generally, most of the video search systems are based on annotations or use additional text information, which is not always of high quality or lack precision. We present a system that allows user to search videos according to their visual content.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A scalable and extensible segment-event-object-based sports video retrieval system</title>
	<abstract>Sport video data is growing rapidly as a result of the maturing digital technologies that support digital video capture, faster data processing, and large storage. However, (1) semi-automatic content extraction and annotation, (2) scalable indexing model, and (3) effective retrieval and browsing, still pose the most challenging problems for maximizing the usage of large video databases. This article will present the findings from a comprehensive work that proposes a scalable and extensible sports video retrieval system with two major contributions in the area of sports video indexing and retrieval. The first contribution is a new sports video indexing model that utilizes semi-schema-based indexing scheme on top of an Object-Relationship approach. This indexing model is scalable and extensible as it enables gradual index construction which is supported by ongoing development of future content extraction algorithms. The second contribution is a set of novel queries which are based on XQuery to generate dynamic and user-oriented summaries and event structures. The proposed sports video retrieval system has been fully implemented and populated with soccer, tennis, swimming, and diving video. The system has been evaluated against 20 users to demonstrate and confirm its feasibility and benefits. The experimental sports genres were specifically selected to represent the four main categories of sports domain: period-, set-point-, time (race)-, and performance-based sports. Thus, the proposed system should be generic and robust for all types of sports.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>State-of-the-art on spatio-temporal information-based video retrieval</title>
	<abstract>Video retrieval is increasingly based on image content. A number of studies on video retrieval have used low-level pixel content related to statistical moments, shape, colour and texture. However, it is well recognised that such information is not enough for uniquely discriminating across different multimedia content. The use of semantic information, especially which derived from spatio-temporal analysis is of great value in multimedia annotation, archiving and retrieval. In this review paper, we detail how the use of spatiotemporal semantic knowledge is changing the way in which modern research the conducted. In this paper we review a number of studies and concepts related to such analysis, and draw important conclusions on where future research is headed.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Content redundancy in YouTube and its application to video tagging</title>
	<abstract>The emergence of large-scale social Web communities has enabled users to share online vast amounts of multimedia content. An analysis of YouTube reveals a high amount of redundancy, in the form of videos with overlapping or duplicated content. We use robust content-based video analysis techniques to detect overlapping sequences between videos. Based on the output of these techniques, we present an in-depth study of duplication and content overlap in YouTube, and analyze various dependencies between content overlap and meta data such as video titles, views, video ratings, and tags. As an application, we show that content-based links provide useful information for generating new tag assignments. We propose different tag propagation methods for automatically obtaining richer video annotations. Experiments on video clustering and classification as well as a user evaluation demonstrate the viability of our approach.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>CLOVIS: towards precision-oriented text-based video retrieval through the unification of automatically-extracted concepts and relations of the visual and audio/speech contents</title>
	<abstract>Traditional multimedia (video) retrieval systems use the keyword-based approach in order to make the search process fast although this approach has several shortcomings and limitations related to the way the user is able to formulate her/his information need. Typical Web multimedia retrieval systems illustrate this paradigm in the sense that the result of a search consists of a collection of thousands of multimedia documents, many of which would be irrelevant or not fully exploited by the typical user. Indeed, according to studies related to users' behavior, an individual is mostly interested in the initial documents returned during a search session and therefore a multimedia retrieval system is to model the multimedia content as precisely as possible to allow for the first retrieved images to be fully relevant to the user's information need. For this, the keyword-based approach proves to be clearly insufficient and the need for a high-level index and query language, addressing the issue of combining modalities within expressive frameworks for video indexing and retrieval is of huge importance and the only solution for achieving significant retrieval performance. This paper presents a multi-facetted conceptual framework integrating multiple characterizations of the visual and audio contents for automatic video retrieval. It relies on an expressive representation formalism handling high-level video descriptions and a full-text query framework in an attempt to operate video indexing and retrieval beyond trivial low-level processes, keyword-annotation frameworks and state-of-the art architectures loosely-coupling visual and audio descriptions. Experiments on the multimedia topic search task of the TRECVID evaluation campaign validate our proposal.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Towards effective video annotation: An approach to automatically link notes with video content</title>
	<abstract>The characteristics of annotations, such as highlighting, context-based notes, and organization are difficult to translate from the traditional paper-based medium to the digital format. An added challenge is how to facilitate annotations on a digital video in a collaborative distance learning environment. To explore issues in video annotation, we developed a tool called Interactive Shared Education Environment (ISEE). ISEE automatically generates hyperlinked timestamps, which we called Smartlinks, to associate the notes with their video contents. A usability study with 59 participants, following up by a small-scale eye-tracking study, was conducted to explore users' video note-taking behaviors and to examine the effect of the new Smartlink design. Our results showed that participants with Smartlink took fewer notes, focused less on video controls and more on video content than those without Smartlink. We believe the main benefit of Smartlink is that it may offload non-learning related cognitive loads and allow users to take better notes. Findings from this study on users' video annotation behaviors shed light on the future design of video annotation systems in both individual and collaborative environments.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>VFerret: content-based similarity search tool for continuous archived video</title>
	<abstract>This paper describes VFerret, a content-based similarity search tool for continuous archived video. Instead of depending on attributes or annotations to search desired data from long-time archived video, our system allows users to perform content-based similarity search using visual and audio features, and to combine content-based similarity search with traditional search methods. Our preliminary experience and evaluation shows that content-based similarity search is easy to use and can achieve 0.79 average precision on our simple benchmark. The system is constructed using Ferret toolkit and its memory footprint for metadata is quite small, requiring about 1.4Gbytes for one year of continuous archived video data.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Experiential meeting system</title>
	<abstract>We are developing experiential meeting systems to allow people to be tele-present in a remote meeting and to be able to review proceedings of a meeting or of several meetings using all the data recorded in a meeting. We consider this as a problem in management and experiential access to all multimedia data acquired in a meeting. The data includes video, audio, presentations, text material, databases and websites related to people and the discussions in the meeting, and any other data or information that could be obtained related to the events in the meeting. For experiential access to live and archived meetings, we propose detecting and storing events at three levels, domain, elemental, and data. We address issues in organizing information at domain level and using current signal processing algorithms for detecting events at data level. We show that to provide better specifications for processing algorithms for video and audio, it is essential to identify what is expected from them and define the environment and expectations very clearly. We also believe that while data processing algorithms are being developed for automatic detection of events, it may be essential to build tagging environments that will allow rapid semiautomatic tagging of data at all three levels so practical meeting systems could be implemented. Use of tagging environment is not only required to enable development of meeting systems in near future, but for defining environments for automatic detection of events in multifarious data. In this paper we present our ideas and experience with different techniques and outline our future directions.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A survey of semantic image and video annotation tools</title>
	<abstract>The availability of semantically annotated image and video assets constitutes a critical prerequisite for the realisation of intelligent knowledge management services pertaining to realistic user needs. Given the extend of the challenges involved in the automatic extraction of such descriptions, manually created metadata play a significant role, further strengthened by their deployment in training and evaluation tasks related to the automatic extraction of content descriptions. The different views taken by the two main approaches towards semantic content description, namely the Semantic Web and MPEG-7, as well as the traits particular to multimedia content due to the multiplicity of information levels involved, have resulted in a variety of image and video annotation tools, adopting varying description aspects. Aiming to provide a common framework of reference and furthermore to highlight open issues, especially with respect to the coverage and the interoperability of the produced metadata, in this chapter we present an overview of the state of the art in image and video annotation tools.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Visualization in Medicine: Theory, Algorithms, and Applications</title>
	<abstract>Visualization in Medicine is the first book on visualization and its application to problems in medical diagnosis, education, and treatment. The book describes the algorithms, the applications and their validation (how reliable are the results?), and the clinical evaluation of the applications (are the techniques useful?). It discusses visualization techniques from research literature as well as the compromises required to solve practical clinical problems. The book covers image acquisition, image analysis, and interaction techniques designed to explore and analyze the data. The final chapter shows how visualization is used for planning liver surgery, one of the most demanding surgical disciplines. The book is based on several years of the authors' teaching and research experience. Both authors have initiated and lead a variety of interdisciplinary projects involving computer scientists and medical doctors, primarily radiologists and surgeons.A core field of visualization and graphics missing a dedicated book until now.Written by pioneers in the field and illustrated in full color. Covers theory as well as practice</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Vlogging: A survey of videoblogging technology on the web</title>
	<abstract>In recent years, blogging has become an exploding passion among Internet communities. By combining the grassroots blogging with the richness of expression available in video, videoblogs (vlogs for short) will be a powerful new media adjunct to our existing televised news sources. Vlogs have gained much attention worldwide, especially with Google's acquisition of YouTube. This article presents a comprehensive survey of videoblogging (vlogging for short) as a new technological trend. We first summarize the technological challenges for vlogging as four key issues that need to be answered. Along with their respective possibilities, we give a review of the currently available techniques and tools supporting vlogging, and envision emerging technological directions for future vlogging. Several multimedia technologies are introduced to empower vlogging technology with better scalability, interactivity, searchability, and accessability, and to potentially reduce the legal, economic, and moral risks of vlogging applications. We also make an in-depth investigation of various vlog mining topics from a research perspective and present several incentive applications such as user-targeted video advertising and collective intelligence gaming. We believe that vlogging and its applications will bring new opportunities and drives to the research in related fields.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Beyond search: Event-driven summarization for web videos</title>
	<abstract>The explosive growth of Web videos brings out the challenge of how to efficiently browse hundreds or even thousands of videos at a glance. Given an event-driven query, social media Web sites usually return a large number of videos that are diverse and noisy in a ranking list. Exploring such results will be time-consuming and thus degrades user experience. This article presents a novel scheme that is able to summarize the content of video search results by mining and threading &#65533;key&#65533; shots, such that users can get an overview of main content of these videos at a glance. The proposed framework mainly comprises four stages. First, given an event query, a set of Web videos is collected associated with their ranking order and tags. Second, key-shots are established and ranked based on near-duplicate keyframe detection and they are threaded in a chronological order. Third, we analyze the tags associated with key-shots. Irrelevant tags are filtered out via a representativeness and descriptiveness analysis, whereas the remaining tags are propagated among key-shots by random walk. Finally, summarization is formulated as an optimization framework that compromises relevance of key-shots and user-defined skimming ratio. We provide two types of summarization: video skimming and visual-textual storyboard. We conduct user studies on twenty event queries for over hundred hours of videos crawled from YouTube. The evaluation demonstrates the feasibility and effectiveness of the proposed solution.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Keeping Found Things Found: The Study and Practice of Personal Information Management: The Study and Practice of Personal Information Management</title>
	<abstract>WE ARE ADRIFT IN A SEA OF INFORMATION. We need information to make good decisions, to get things done, to learn, and to gain better mastery of the world around us. But we do not always have good control of our information - not even in the "home waters" of an office or on the hard drive of a computer. Instead, information may be controlling us - keeping us from doing the things we need to do, getting us to waste money and precious time. The growth of available information, plus the technologies for its creation, storage, retrieval, distribution and use, is astonishing and sometimes bewildering. Can there be a similar growth in our understanding for how best to manage information and informational tools?. This book provides a comprehensive overview of personal information management (PIM) as both a study and a practice of the activities people do and need to be doing so that information can work for them in their daily lives. Introductory chapters of Keeping Found Things Found: The Study and Practice of Personal Information Management provide an overview of PIM and a sense for its many facets. The next chapters look more closely at the essential challenges of PIM, including finding, keeping, organizing, maintaining, managing privacy, and managing information flow. The book also contains chapters on search, email, mobile PIM, web-based support, and other technologies relevant to PIM.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A survey of traceability in requirements engineering and model-driven development</title>
	<abstract>Traceability--the ability to follow the life of software artifacts--is a topic of great interest to software developers in general, and to requirements engineers and model-driven developers in particular. This article aims to bring those stakeholders together by providing an overview of the current state of traceability research and practice in both areas. As part of an extensive literature survey, we identify commonalities and differences in these areas and uncover several unresolved challenges which affect both domains. A good common foundation for further advances regarding these challenges appears to be a combination of the formal basis and the automated recording opportunities of MDD on the one hand, and the more holistic view of traceability in the requirements engineering domain on the other hand.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Smart meeting systems: A survey of state-of-the-art and open issues</title>
	<abstract>Smart meeting systems, which record meetings and analyze the generated audio--visual content for future viewing, have been a topic of great interest in recent years. A successful smart meeting system relies on various technologies, ranging from devices and algorithms to architecture. This article presents a condensed survey of existing research and technologies, including smart meeting system architecture, meeting capture, meeting recognition, semantic processing, and evaluation methods. It aims at providing an overview of underlying technologies to help understand the key design issues of such systems. This article also describes various open issues as possible ways to extend the capabilities of current smart meeting systems.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Experiences of semantic tagging with Tilkut</title>
	<abstract>There are two main approaches for adding metadata to web content: free user-generated tags and keywords based on taxonomies or semantic ontologies. There are pros and cons in both approaches, so benefits may be gained by combining these approaches. This paper presents a solution that utilizes both freely defined tags and predefined ontologies. To study suggestionbased semantic tagging we created a social bookmarking application called Tilkut. This paper describes the system and experiences from a small scale user study. Suggestions for more intelligent tagging solutions will be given.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>GAT: a Graphical Annotation Tool for semantic regions</title>
	<abstract>This article presents GAT, a Graphical Annotation Tool based on a region-based hierarchical representation of images. The proposed solution uses Partition Trees to navigate through the image segments which are automatically defined at different spatial scales. Moreover, the system focuses on the navigation through ontologies for a semantic annotation of objects and of the parts that compose them. The tool has been designed under usability criteria to minimize the user interaction by trying to predict the future selection of regions and semantic classes. The implementation uses MPEG-7/XML input and output data to allow interoperability with any type of Partition Tree. This tool is publicly available and its source code can be downloaded under a free software license.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Text mining and probabilistic language modeling for online review spam detection</title>
	<abstract>In the era of Web 2.0, huge volumes of consumer reviews are posted to the Internet every day. Manual approaches to detecting and analyzing fake reviews (i.e., spam) are not practical due to the problem of information overload. However, the design and development of automated methods of detecting fake reviews is a challenging research problem. The main reason is that fake reviews are specifically composed to mislead readers, so they may appear the same as legitimate reviews (i.e., ham). As a result, discriminatory features that would enable individual reviews to be classified as spam or ham may not be available. Guided by the design science research methodology, the main contribution of this study is the design and instantiation of novel computational models for detecting fake reviews. In particular, a novel text mining model is developed and integrated into a semantic language model for the detection of untruthful reviews. The models are then evaluated based on a real-world dataset collected from amazon.com. The results of our experiments confirm that the proposed models outperform other well-known baseline models in detecting fake reviews. To the best of our knowledge, the work discussed in this article represents the first successful attempt to apply text mining methods and semantic language models to the detection of fake consumer reviews. A managerial implication of our research is that firms can apply our design artifacts to monitor online consumer reviews to develop effective marketing or product design strategies based on genuine consumer feedback posted to the Internet.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Automatic creation of photo books from stories in social media</title>
	<abstract>Photos are a special way to tell stories of our best memories and moments. The representation of those photos in appealing physical photo books is highly appreciated by many people. Today, many photos are shared via social networking sites, where people upload their photos and share their stories with their friends. The members of social networks comment on each other's photos, add tags or descriptions and upload new photos of the same events to their albums. While the media of different personal events are available on the social network, there is no easy way to collect and bundle them into a story and print this story as a photo book. We propose an approach to automatically detect media elements that match a query (where, when, what, who) in the user's social network and intelligently arrange and compose them into a printable photo book. We combine content analysis of text and images to automatically and semi-automatically select photos of a specific story. We calculate the probabilities of each two photos to belong to the same event using an Expectation-Maximization algorithm that we propose in order to be able to retrieve them easily when receiving the user queries, and we address the differences between our model and other models that use similar proposed algorithms. People's tags and the interaction between the users and the photos as well as other semantic information are exploited to select important photos that are suitable to create the photo book. The selected photos and derived semantics are then employed to automatically create an appealing layout for the photo book.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Parsed use case descriptions as a basis for object-oriented class model generation</title>
	<abstract>Object-oriented analysis and design has become a major approach in the design of software systems. Recent developments in CASE tools provide help in documenting the analysis and design stages and in detecting incompleteness and inconsistency in analysis. However, these tools do not contribute to the initial and difficult stage of the analysis process of identifying the objects/classes, attributes and relationships used to model the problem domain. This paper presents a tool, Class-Gen, which can partially automate the identification of objects/classes from natural language requirement specifications for object identification. Use case descriptions (UCDs) provide the input to Class-Gen which parses and analyzes the text written in English. A parsed use case description (PUCD) is generated which is then used as the basis for the construction of an initial UML class model representing object classes and relationships identified in the requirements. PUCDs enable the extraction of nouns, verbs, adjectives and adverbs from traditional UCDs for the identification process. Finally Class-Gen allows the initial class model to be refined manually. Class-Gen has been evaluated against a collection of unseen requirements. The results of the evaluation are encouraging as they demonstrate the potential for such tools to assist with the software development process.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Building a Usable and Accessible Semantic Web Interaction Platform</title>
	<abstract>Semantic Web applications take off is being slower than expected, at least with respect to "real-world" applications and users. One of the main reasons for this lack of adoption is that most Semantic Web user interfaces are still immature from the usability and accessibility points of view. This is due to the novelty of these technologies, but this also motivates the exploration of alternative interaction paradigms, different from the "traditional" Web or Desktop applications ones. Our proposal is realized in the Rhizomer platform, which explores the possibilities of the object---action interaction paradigm at the Web scale. This paradigm is well suited for heterogeneous resource spaces such as those common in the Semantic Web. Resources, described by metadata, correspond to the objects in the paradigm. Semantic web services, which are dynamically associated to these objects, correspond to the actions. The platform is being put into practice in the context of a research project in order to build an open application for media distribution based on Semantic Web technologies. Moreover, its usability and accessibility have been evaluated in this real setting and compared to similar systems.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>MonuAnno: automatic annotation of georeferenced landmarks images</title>
	<abstract>Uploading tourist photographs is a popular activity on photo sharing platforms. The manual annotation of these images is a tedious process and the users often upload their images with no associated textual information. Automating the annotation process has received a lot of attention but the problem remains a hard one, especially when dealing with large and heterogeneous databases. Here we focus on landmarks images, very frequent among tourism pictures, and propose a new automatic technique for annotating this type of pictures. Our system, called MonuAnno, relies on the joint exploitation of localization information and of image content analysis in an efficient and scalable framework. The annotation is performed using a two steps k Nearest Neighbors (k-NN). First, only neighboring landmarks of a new unlabeled georeferenced image will be considered as potential annotations and the image will be attributed to the landmark that is visually closest. Then, we introduce a verification step that eliminates false positives (images taken near a landmark that represent something else). The technique was tested on Web images and the results show that the precision of the labeling process in MonuAnno exceeds 80%, when annotating around 50% of the images in the test set.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Model-based analysis of flow-mediated dilation and intima-media thickness</title>
	<abstract>We present an end-to-end system for the automatic measurement of flow-mediated dilation (FMD) and intima-media thickness (IMT) for the assessment of the arterial function. The video sequences are acquired from a B-mode echographic scanner. A spline model (deformable template) is fitted to the data to detect the artery boundaries and track them all along the video sequence. The a priori knowledge about the image features and its content is exploited. Preprocessing is performed to improve both the visual quality of video frames for visual inspection and the performance of the segmentation algorithm without affecting the accuracy of the measurements. The system allows real-time processing as well as a high level of interactivity with the user. This is obtained by a graphical user interface (GUI) enabling the cardiologist to supervise the whole process and to eventually reset the contour extraction at any point in time. The system was validated and the accuracy, reproducibility, and repeatability of the measurements were assessed with extensive in vivo experiments. Jointly with the user friendliness, low cost, and robustness, this makes the system suitable for both research and daily clinical use.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Automatic tag expansion using visual similarity for photo sharing websites</title>
	<abstract>In this paper we present an automatic photo tag expansion method designed for photo sharing websites. The purpose of the method is to suggest tags that are relevant to the visual content of a given photo at upload time. Both textual and visual cues are used in the process of tag expansion. When a photo is to be uploaded, the system asks for a couple of initial tags from the user. The initial tags are used to retrieve relevant photos together with their tags. These photos are assumed to be potentially content related to the uploaded target photo. The tag sets of the relevant photos are used to form the candidate tag list, and visual similarities between the target photo and relevant photos are used to give weights to these candidate tags. Tags with the highest weights are suggested to the user. The method is applied on Flickr ( http://www.flickr.com ). Results show that including visual information in the process of photo tagging increases accuracy with respect to text-based methods.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Frankenrigs: building character rigs from multiple sources</title>
	<abstract>We present a new rigging and skinning method which uses a database of partial rigs extracted from a set of source characters. Given a target mesh and a set of joint locations, our system can automatically scan through the database to find the best-fitting body parts, tailor them to match the target mesh, and transfer their skinning information onto the new character. For the cases where our automatic procedure fails, we provide an intuitive set of tools to fix the problems. When used fully automatically, the system can generate results of much higher quality than a standard smooth bind, and with some user interaction, it can create rigs approaching the quality of artist-created manual rigs in a small fraction of the time.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Medical Image Analysis: Progress over Two Decades and the Challenges Ahead</title>
	<abstract>The analysis of medical images has been woven into the fabric of the Pattern Analysis and Machine Intelligence (PAMI) community since the earliest days of these Transactions. Initially, the efforts in this area were seen as applying pattern analysis and computer vision techniques to another interesting dataset. However, over the last two to three decades, the unique nature of the problems presented within this area of study have led to the development of a new discipline in its own right. Examples of these include: the types of image information that are acquired, the fully three-dimensional image data, the nonrigid nature of object motion and deformation, and the statistical variation of both the underlying normal and abnormal ground truth. In this paper, we look at progress in the field over the last 20 years and suggest some of the challenges that remain for the years to come.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Assisted animated production creation and programme generation</title>
	<abstract>The creation of animated productions is a labour intensive process. Whether the end result is a large-budget motion picture, or a small-scale internet production, there is invariably a large amount of time spent in creating the timeline, arranging assets, previewing and editing. This iterative process is necessary in large-scale productions but can become repetitive and frustrating when the end result is a small production that may have similar elements to previous work. We present a workflow system and framework that are able to both greatly facilitate animated programme production and introduce an element of procedural generation. We further present the Programme Editor, an application designed to be a powerful front end for the framework. The principal contribution of this work is the creation of an XML-based scripting engine that can be used to create an animated production. This permits several techniques, tools and workflows to interchange information, allows rapid incorporation of further tools, and furthermore facilitates the complete automatisation of the production process.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Enhancing search in a geospatial multimedia annotation system</title>
	<abstract>The development of numerous information sharing platforms have led to the emergence of multimedia user-generated content. With the prevalence of networking mobile devices and Global Positioning System (GPS) functionality, these contents could also be tagged with their location (geo-tagged) and visualised on a map through a geospatial information system. This in turn raises new challenges to manage the information retrieval process due to the potentially large amounts of data presented on the map. In this paper, we present a spatial clustering approach to enhance the searching feature of MobiTOP, a geospatial annotation system. The technique, which is a modification of DBSCAN (Density based spatial clustering applications with noise) coupled with our ranking schemes, is compared against other techniques. The evaluation results suggest the viability of our approach, and implications are also discussed.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>3</relevance>
  </item>
  <item>
    <title>Towards the semantic and context-aware management of mobile multimedia</title>
	<abstract>Users of mobile devices can nowadays easily create large quantities of mobile multimedia documents tracing significant events attended, places visited or, simply, moments of their everyday life. However, they face the challenge of organizing these documents in order to facilitate searching through them at a later time and sharing them with other users. We propose using context awareness and semantic technologies in order to improve and facilitate the organization, annotation, retrieval and sharing of personal mobile multimedia documents. Our approach combines metadata extracted and enriched automatically from the users' context with annotations provided manually by the users and with annotations inferred by applying user-defined rules to context features. These new contextual metadata are integrated into the processes of annotation, sharing and keyword-based retrieval.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A hybrid ontology and visual-based retrieval model for cultural heritage multimedia collections</title>
	<abstract>This paper introduces a hybrid multimedia retrieval model that is capable of retrieving cultural heritage multimedia content, based on their semantic annotation with the help of an ontology and on low level visual features with a view to finding similar content. The main novelty is the way in which these techniques cooperate transparently during the evaluation of a single query in a hybrid fashion, making recommendations to the user. A search engine has been developed implementing this model, which is capable of searching through cultural heritage multimedia collections, and indicative examples are discussed, along with insights into its performance.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A web-based system for collaborative annotation of large image and video collections: an evaluation and user study</title>
	<abstract>Annotated collections of images and videos are a necessary basis for the successful development of multimedia retrieval systems. The underlying models of such systems rely heavily on quality and availability of large training collections. The annotation of large collections, however, is a time-consuming and error prone task as it has to be performed by human annotators. In this paper we present the IBM Efficient Video Annotation (EVA) system, a server-based tool for semantic concept annotation of large video and image collections. It is optimised for collaborative annotation and includes features such as workload sharing and support in conducting inter-annotator analysis. We discuss initial results of an ongoing user-evaluation of this system. The results are based on data collected during the 2005 TRECVID Annotation Forum, where more than 100 annotators have been using the system.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Multimedia ontology learning for automatic annotation and video browsing</title>
	<abstract>In this work, we offer an approach to combine standard multimedia analysis techniques with knowledge drawn from conceptual metadata provided by domain experts of a specialized scholarly domain, to learn a domain-specific multimedia ontology from a set of annotated examples. A standard Bayesian network learning algorithm that learns structure and parameters of a Bayesian network is extended to include media observables in the learning. An expert group provides domain knowledge to construct a basic ontology of the domain as well as to annotate a set of training videos. These annotations help derive the associations between high-level semantic concepts of the domain and low-level MPEG-7 based features representing audio-visual content of the videos. We construct a more robust and refined version of this ontology by learning from this set of conceptually annotated videos. To encode this knowledge, we use MOWL, a multimedia extension of Web Ontology Language (OWL) which is capable of describing domain concepts in terms of their media properties and of capturing the inherent uncertainties involved. We use the ontology specified knowledge for recognizing concepts relevant to a video to annotate fresh addition to the video database with relevant concepts in the ontology. These conceptual annotations are used to create hyperlinks in the video collection, to provide an effective video browsing interface to the user.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Automatic annotation of multimedia content by user clickthroughs: enhancing the performance of multimedia search engines</title>
	<abstract>Content-based multimedia retrieval is a very hot research topic, applicable to several domains. Traditional feature vector based retrieval methods cannot provide semantically meaningful results. Additionally manual annotation is both time/money consuming and user-dependent. To address these problems in this paper we present an approach to automatically annotate multimedia files by incorporating clickthrough data of search engines. In particular the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking, are analyzed in order to assign keywords to selected content. A query extension method is also proposed in order to agitate the pool of files and bring content with similar visual features to the surface. This is very important since users typically select only the first files of the ranking by clicking on them. The proposed method is feasible even for large sets of queries and features and theoretical results are verified in a controlled experiment, which shows that the method can effectively annotate multimedia files and significantly enhance the performance of multimedia search engines.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Learning ontology for personalized video retrieval</title>
	<abstract>This paper proposes a new method for using implicit user feedback from clickthrough data to provide personalized ranking of results in a video retrieval system. The annotation based search is complemented with a feature based ranking in our approach. The ranking algorithm uses belief revision in a Bayesian Network, which is derived from a multimedia ontology that captures the probabilistic association of a concept with expected video features. We have developed a content model for videos using discrete feature states to enable Bayesian reasoning and to alleviate on-line feature processing overheads. We propose a reinforcement learning algorithm for the parameters of the Bayesian Network with the implicit feedback obtained from the clickthrough data.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Investigating the usability of a mobile location-based annotation system</title>
	<abstract>We investigate the usability of MobiTOP (Mobile Tagging of Objects and People), a mobile location-based annotation system. MobiTOP allows users to annotate real world locations with both multimedia and textual content and concurrently, share the annotations among its users. Equipped with a new interface as well as additional functionality such as clustering of annotations and advanced search and filtering options, a usability evaluation of MobiTOP was conducted in the context of a travel companion for tourists. The results suggested the potential of the system in terms of functionality for mobile content sharing. Participants agree that the features in MobiTOP are generally usable as a content sharing tool. Implications and future work are also reported in this paper.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Relevance feedback strategies for artistic image collections tagging</title>
	<abstract>This paper provides an analysis on relevance feedback techniques in a multimedia system designed for the interactive exploration and annotation of artistic collections, in particular illuminated manuscripts. The relevance feedback is presented not only as a very effective technique to improve the performance of the system, but also as a clever way to increase the user experience, mixing the interactive surfing through the artistic content with the possibility to gather valuable information from the user, and consequently improving his retrieval satisfaction. We compare a modification of the Mean-Shift Feature Space Warping algorithm, as representative of the standard RF procedures, and a learning-based technique based on transduction, considered in order to overcome some limitation of the previous technique. Experiments are reported regarding the adopted visual features based on covariance matrices.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A usability study of a mobile content sharing system</title>
	<abstract>We investigate the usability of MobiTOP (Mobile Tagging of Objects and People), a mobile location-based content sharing system. MobiTOP allows users to annotate real world locations with both multimedia and textual content and concurrently, share the annotations among its users. In addition, MobiTOP provides additional functionality such as clustering of annotations and advanced search and filtering options. A usability evaluation of the system was conducted in the context of a travel companion for tourists. The results suggested the potential of the system in terms of functionality for mobile content sharing. Participants agreed that the features in MobiTOP were generally usable as a content sharing tool. Implications and future work are also reported in this paper.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Image retrieval based on a multipurpose watermarking scheme</title>
	<abstract>The rapid development of Internet and multimedia technologies has made copyright protection and multimedia retrieval be the two most important issues in the digital world. To solve these problems simultaneously, this paper presents a multipurpose watermarking scheme for image retrieval. First, several important features are computed offline for each image in the database. Then, the copyright, annotation and feature watermarks are offline embedded into all images in the database. During the online retrieval, the query image features are compared with the exacted features from each image in the database to find the similar images. The experimental results based on a database with 1000 images in 10 classes demonstrate the effectiveness of the proposed scheme.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Surfing on artistic documents with visually assisted tagging</title>
	<abstract>This paper describes a complete architecture for the interactive exploration and annotation of artistic collections. In particular the focus is on Renaissance illuminated manuscripts, which typically contain thousands of pictures, used to comment or embellish the manuscript Gothic text. The final aim is to create a human centered multimedia application allowing the non practitioners to enjoy these masterpieces and expert users to share their knowledge. The system is composed by a modern user interface for browsing, surfing and querying, an automatic segmentation module, to ease the initial picture extraction task, and a similarity based retrieval engine, used to provide visually assisted tagging capabilities. A relevance feedback procedure is included to further refine the results. Experiments are reported regarding the adopted visual features based on covariance matrices and the Mean Shift Feature Space Warping relevance feedback. Finally some hints on the user interface for museum installations are discussed.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>How do you feel about "dancing queen"?: deriving mood And theme annotations from user tags</title>
	<abstract>Web 2.0 enables information sharing, collaboration among users and most notably supports active participation and creativity of the users. As a result, a huge amount of manually created metadata describing all kinds of resources is now available. Such semantically rich user generated annotations are especially valuable for digital libraries covering multimedia resources such as music, where these metadata enable retrieval relying not only on content-based (low level) features, but also on the textual descriptions represented by tags. However, if we analyze the annotations users generate for music tracks, we find them heavily biased towards genre. Previous work investigating the types of user provided annotations for music tracks showed that the types of tags which would be really beneficial for supporting retrieval - usage (theme) and opinion (mood) tags - are often neglected by users in the annotation rocess. In this paper we address exactly this problem: in order to support users in tagging and to fill these gaps in the tag space, we develop algorithms for recommending mood and theme annotations. Our methods exploit the available user annotations, the lyrics of music tracks, as well as combinations of both. We also compare the results for our recommended mood / theme annotations against genre and style recommendations - a much easier and already studied task. Besides evaluating against an expert (AllMusic.com) ground truth, we evaluate the quality of our recommended tags through a Facebook-based user study. Our results are very promising both in comparison to experts as well as users and provide interesting insights into possible extensions for music tagging systems to support music search.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Personalized video adaptation based on video content analysis</title>
	<abstract>Personalized video adaptation is expected to satisfy individual users' needs on video content. Multimedia data mining plays a significant role of video annotation to meet users' preference on video content. In this paper, a comprehensive solution for personalized video adaptation is proposed based on video content mining. Video content mining targets both cognitive content and affective content. Cognitive content refers to those semantic events, which are very specific for the video domains. Sometimes, users might prefer "emotional decision" to select their interested video content. Therefore, we introduce affective content which causes audiences' strong reactions.
For cognitive content mining, features are extracted from multiple modalities. Machine learning module is further performed to get some middle-level features, such as specific audio sounds, semantic video shots and so on. Those middle-level features are used to detect cognitive content by using Hidden Markov Models. For affective content mining, affective content is detected with three affective levels: "low", "medium" and "high". Considering affective levels might have no sharp boundaries, fuzzy c mean clustering is used on low-level features to simulate user's perceptions.
The adaptation is later implemented based on MPEG-21 Digital Item Adaptation framework. One of the challenges is how to quantify users' preference on video content. Information Entropy (IE) and Membership Functions are calculated to decide priorities for resource allocation for cognitive content and affective content respectively.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Content-based mood classification for photos and music: a generic multi-modal classification framework and evaluation approach</title>
	<abstract />
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Timesheets.js: when SMIL meets HTML5 and CSS3</title>
	<abstract>In this paper, we explore different ways to publish multimedia documents on the web. We propose a solution that takes advantage of the new multimedia features of web standards, namely HTML5 and CSS3. While JavaScript is fine for handling timing, synchronization and user interaction in specific multimedia pages, we advocate a more generic, document-oriented alternative relying primarily on declarative standards: HTML5 and CSS3 complemented by SMIL Timesheets. This approach is made possible by a Timesheets scheduler that runs in the browser. Various applications based on this solution illustrate the paper, ranging from media annotations to web documentaries.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Concept based interactive retrieval for social environment</title>
	<abstract>Following the recent developments in social networking, there is an emerging interest to share experiences online with social peers through multimedia data. Consequently, exponential amount of multimedia information has been generated by everyday users and shared among social peers. As opposed to conventional digital archives, the user generated content archive does not confine to one particular domain and therefore semantic indexing of the content requires the creation of large number of training samples for each semantic query concept. Addressing this problem, we present an interactive multi-concept based browsing and retrieval framework using which users can construct high-level semantic queries based on mid-level primitive features. The proposed framework integrates innovative visualisation methodology developed for browsing, navigating and retrieving information from multimedia database. The framework is user centric and supports interactive formulation of high-level semantic queries for content retrieval using available content annotation. The performance of the proposed framework is evaluated using annotation based on automatic algorithms against Support Vector Machines, Multi-feature classification and particle swarm optimisation based relevance feedback techniques.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Browsing personal digital photograph collections with spatial and temporal based ontology and MPEG-7 dozen dimensional digital content architecture</title>
	<abstract>The current trend of image retrieval is to incorporate the image visual features used in Content Based Image Retrieval (CBIR) and semantics annotations used in Metadata Based Image Retrieval to enhance retrieval performance. Because of the pervasive of consumer imaging devices, building personal digital photograph libraries became an increasingly interested domain. Personal digital photograph collections have specific characteristics compare to general purpose image databases. Hence, annotation architecture specially designed for that plays an important role in building an interoperatable data repository for future indexing, browsing and retrieving purposes. We propose a MPEG- 7 based multimedia content description architecture, Dozen Dimensional Digital Content (DDDC), which annotates multimedia data with twelve main attributes regarding its semantic representation. In addition, we also proposed a machine-understandable "Spatial and Temporal Based Ontology" representation for the above DDDC semantics description to enable semi-automatic annotation process.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A video retrieval system for electrical safety education based on a mobile agent</title>
	<abstract>Recently, retrieval of various video data has become an important issue as more and more multimedia content services are being provided. To effectively deal with video data, a semantic-based retrieval scheme that allows for processing diverse user queries and saving them on the database is required. In this regard, this paper proposes a semantic-based video retrieval system that allows the user to search diverse meanings of video data for electrical safety-related educational purposes by means of automatic annotation processing. If the user inputs a keyword to search video data for electrical safety-related educational purposes, the mobile agent of the proposed system extracts the features of the video data that are afterwards learned in a continuous manner, and detailed information on electrical safety education is saved on the database. The proposed system is designed to enhance video data retrieval efficiency for electrical safety-related educational purposes.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Shiatsu: semantic-based hierarchical automatic tagging of videos by segmentation using cuts</title>
	<abstract>The recent dramatic widespread of multimedia content over the Internet and other media channels (such as television or mobile phone platforms) points the interest of media broadcasters to the topics of video retrieval and content browsing. Semantic indexing based on content is a good starting point for an effective retrieval system, since it allows an intuitive categorization of videos. However, the annotation process is usually done manually, leading to ambiguity, lack of information, and translation problems. In this paper, we propose SHIATSU, a novel technique for automatic video tagging which is based on shot boundaries detection and hierarchical annotation processes. Our shot detection module uses a simple yet efficient algorithm based on HSV histograms and edge features. The tagging module assigns semantic concepts to both shot sequences and whole videos, by exploiting visual features extracted from key frames. We present preliminary results of our technique on the Mammie platform video set by proving its effectiveness in real scenarios.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>

  </results>